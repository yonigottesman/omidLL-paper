We now describe \sysll, a scalable low-latency TPS algorithm 
% on top of HBase. 
%\sys\ is an evolution of the open-source Omid TPS, redesigned to reduce latency.
%For clarity of the presentation, in this section we describe \sys\ without the fast-path; 
%This protocol 
that satisfies standard (unrelaxed) SI semantics and is amenable to multi-tenancy
\footnote{\small{\url{https://issues.apache.org/jira/browse/OMID-90} summarizes 
the \sysll\/ development.}}.
We saw above that, while many TPSs follow a similar schema,  they make different design choices when implementing this schema. 
We overview our design choices in Section~\ref{ssec:ll-txns}. 
We then proceed to give a detailed description of the protocol
in Section~\ref{ssec:ll}.

\subsection{\sysll\ design choices}
\label{ssec:ll-txns}

We now discuss our design choices, which are geared towards high performance without sacrificing cloud deployability.
Table~\ref{table:design-space} compares them with choices made in other TPSs. 
We are not familiar with another TPS that makes the same design choices as \sysll. 


\begin{table*}[htb]
\centerline{
\begin{tabular}{|l|cccc|}
% data store modification & 
\hline
TPS & validation & commit  entry updates	&  multi-tenancy &
 read may cause abort  \\
\hline
Percolator, \syspc & {\bf D} & {\bf D} & no & yes \\
CockroachDB	& {\bf D}      & {\bf D} & yes & yes \\
Omid1, Tephra 	& {\bf C}      & {\bf R} & yes & no \\
Omid  		& {\bf C}      & {\bf C} & yes & no \\
{\bf \sysll }		& {\bf C}      & {\bf D} & {\bf yes} & {\bf yes} \\
% \sys			& {\bf C} & {\bf D} & {\bf D} & yes& yes\\
\hline
\end{tabular}
}
\caption{Design choices  in TPSs. {\bf C} -- centralized,  {\bf D} -- distributed, {\bf R} -- replicated.}
\label{table:design-space}
\end{table*}



\mypara{Centralized validation.}
\sysll\ adopts Omid's centralized conflict detection mechanism, which eliminates the need for locking objects
in the data store, and is extremely scalable~\cite{Omid2017}. 

Other TPSs (like Percolator and  CockroachDB~\cite{cockroach}) instead use a distributed 2PC-like protocol that locks all written objects during validation (either at commit time or during the write). To this end, they use atomic check\&mutate operations on the underlying data store. This slows down either commits (in case of commit-time validation) or transactional writes (in case of write-time validation), which takes a toll on long transactions, where validation time is substantial. 
\remove{
%that can abort either the current transaction or a conflicting one. 
Moreover, it may hold locks for a long period (in case of a long transaction),
during which conflicts lead to aborts.  
}
%
To allow us to compare the two approaches, we also implement a 2PC-based version of Omid, \syspc,
which follows Percolator's design.
%Furthermore, 
%this approach does not entail any changes to the underlying data store (HBase in our implementation),
%which can facilitate production deployment.
%Using check\&mutate for all updates makes data store writes slower,

\mypara{Distributed commit entry updates with  multi-tenancy.}
%
The early generation of Omid~\cite{OmidICDE2014} (referred to as Omid1) and Tephra replicate commit entries 
of pending transactions among all active clients, which consumes high bandwidth and does not scale. Omid 
%and CockroachDB 
instead uses a dedicated table, and 
%CockroachDB updates the table in a distributed manner, while Omid 
has the centralized TM persist all commits to this table. 
Our experiments show that the centralized access to commit entries is Omid's main scalability bottleneck, 
and while this bottleneck is mitigated via batching, this also increases latency.
Omid chose this  option as it was designed for high throughput. 

Here, on the other hand, we target  low latency. 
We therefore distribute the commit entry updates, and allow commit entries of different transactions to be 
updated in parallel by independent clients. We will see below that this modification reduces latency
by up to an order of magnitude for small transactions.

We note that commit table updates are distributed also in CockroachDB and Percolator. 
The latter takes this approach one step further, and distributes not only the commit table updates 
but also the actual commit entries. There, commit entries reside in user data tables, 
where the first row written in a given transaction holds the commit entry for that transaction.
The problem with this approach is that it assumes that all clients have permissions to access all 
tables. For example, a transaction attempting to read from  table $A$ may encounter a write intent 
produced by a transaction that accessed table $B$ before table $A$, and will need to refer to 
that transaction's commit entry in table $B$ in order to determine its status. This approach 
did not pose a problem in Percolator, which was designed for use in a single application (Google Search), 
but is unacceptable in multi-tenant settings.   

Unlike data tables, which are owned by individual applications that manage their permissions,  
the dedicated commit table is owned by the TPS; it is accessed exclusively by the 
TPS client library, which in turn is only invoked   internally by the database engine, and not by application code. 

{\inred{The commit table is a highly contended resource. While it is not very big at any given time, it is accessed
by every single transaction that modifies data. In large clusters the update rate may become huge (see Section~\ref{sec:eval}).
It is therefore imperative to shard this table evenly across many nodes, in order to spread the load.}}

\mypara{Write intent resolution.}
As in other TPSs, reads resolve write intents via the commit entry.
If the transaction status is committed, the commit time is checked, and, if smaller than or equal to $ts_r$, it is taken into account;
if the transaction is aborted, the value is ignored.
In case the transaction status is pending, \sys, like Percolator and CockroachDB, has the reader
force the writing transaction to abort. This is done using an atomic check\&mutate operation to set the status in the
writing transaction's commit entry to aborted.

Omid and Tephra, on the other hand, do not need to force such aborts\footnote{Omid's high availability mechanism  
may force such aborts in rare cases of TM failover.}, because they ensure that  if the read sees a write intent
by an uncommitted transaction, the latter will not commit with an earlier timestamp than the read.
This is achieved  
%a single TM that allocates timestamps, performs validation, and writes the commit entry.
either by delaying begin and commit requests until all transactions that concurrently attempt to commit complete (as in Omid),
or sending information about all these transactions to the beginning client (as in Omid1 and Tephra).
\sysll\ avoids such costly mechanisms by allowing reads to force aborts. 



\remove{

The centralized service, called Transactional Status Oracle (TSO), maintains an in-memory hash table mapping keys to 
commit timestamps ($ts_c$) of transactions that last wrote them. A conflict arises whenever a key in a transaction's write-set has been written 
with a timestamp higher than its $ts_r$. 
In case the conflict detection service crashes, all pending transactions are aborted, and it can be immediately restarted with an empty table, because only conflicts with concurrent transactions need to be checked. 


%
The possibility of reads aborting pending transactions means that commit attempts (line~\ref{l:commit}) have to  
check whether the transaction is aborted atomically with writing the commit status.


\subsection{Local transactions in \sys}
\label{ssec:fp-impl}
To support local transactions, we implemented the algorithms and APIs described in Section~\ref{sec:alg}. Each Hbase region server maintains an LVC which is updated whenever a transaction accesses it. Omid's API was extended to support FP transactions, and the write stage in regular transactions was modified to use checkAndMutate instead of put. 


 }
 