

We describe our methodology and experiment setup in Section~\ref{ssec:methodology}, and present our results 
in Section~\ref{ssec:results}.

\subsection{Methodology}
\label{ssec:methodology}

\paragraph{Evaluated systems.}

We compare \sys\  to  state-of-the art TPSs and separately evaluate the effectiveness of its FP mechanism.
To this end, we compare four TPSs, all of which use HBase as the underlying data store:
\begin{description}
\item[Vanilla \sys] -- the algorithm described in Section~\ref{sec:ll}, without support for FP transactions.
\item[\sys] -- single-key transactions use the FP API, 
whereas longer transactions use the standard API, 
and their transactional reads and writes are modified to access the LVC along with the data as described in Section~\ref{sec:alg}.
\item[Omid] -- the Apache Incubator version of Omid, on which we base our implementation of \sys. 
\item[2PC] -- an implementation (in the \sys\ framework) of Percolator's concurrency control mechanism:
Begins and commits use the TM only for timestamp allocation and not for conflict detection; conflicts are detected via 
\emph{Two Phase Commit (2PC)}.
\end{description}

Direct comparison to CockroachDB is not feasible because CockroachDB supports SQL transactions over a multi-tier
architecture using various components that are incompatible with HBase. 
We do no compare \sys\ to Omid1 (and the similarly-designed Tephra), since Omid significantly
outperforms Omid1~\cite{Omid2017}.

To reduce latency, we configure the TPSs to perform the post-commit phase asynchronously, 
by a separate thread, after the transaction completes.

\remove{
In Omid and Vanilla \sys, all transactions use the standard API, namely 
delineating transactions with begin and commit calls.
In FP \sys, transactions that can use the FP API (specifically, singleton reads, singleton writes, 
and single-key read-writes) do so, whereas other transactions use the standard API.
}

\paragraph{Experiment setup.}

Our experiment testbed consists of nine 12-core Intel Xeon 5 machines with 46GB RAM and 4TB 
SSD storage, interconnected by 10G Ethernet. We allocate three of these to HBase nodes, 
one to the TM, one to emulate the client whose performance we measure, and four more to simulate 
background traffic as explained below. Each HBase node runs both an HBase region server and 
the underlying Hadoop File System (HDFS) server within 8GB JVM containers. 

Note that the only scalability bottleneck in the tested systems is the centralized TM.
HBase, on the other hand, scales horizontally across thousands of nodes, each of which processes a small fraction of the total load. 
Since each node typically serves millions of  keys, data access rates remain load-balanced across nodes 
even when access to individual keys is highly skewed.  And since read/write requests are processed independently by each HBase node, 
their performance remains constant as the workload and  system size are scaled by the same factor. 

Thus, to understand the system's performance at scale, 
we can run transactions over a small HBase cluster with an appropriately-scaled load, 
but need to stress the TM as a bigger deployment would. 
We do this at a ratio of 3:1000; that is, we run transactions on a $3$-node HBase cluster and 
load the TM with a begin/commit request rate that would arise in a $1000$-node HBase cluster with the same per-node load.
For example, to test the client's latency at $100$K tps, we have the TM handle $100$K tps, and have  an HBase deployment of
three nodes handle read/write requests of $0.3$K tps. As explained above, 
the HBase latency at $100$K tps with $1000$ servers would be roughly the same as in this deployment.

We use four machines to generate the background load on the TM using a custom tool~\cite{Omid2017} 
that asynchronously posts begin and commit requests on the wire and collects the TM's responses. 
We note that although in this experiment the TM maintains a small number of client connections (serving many requests per connection), 
the number in a true $1000$-node system still falls well within the OS limit, hence no real bottleneck is ignored. 

We measure the end-to-end client-incurred latency on a single client node that generates transactions over the $3$-server HBase cluster. 
Note that the client also generates begin/commit requests, which account for \mbox{$\sim0.3\%$} of the TM's load.
The client runs the popular YCSB benchmark~\cite{Cooper:2010:BCS:1807128.1807152}, 
exercising the synchronous transaction processing API in a varying number of threads. 

\paragraph{Workload.}

Our test cluster stores approximately 23M keys ($\sim\!\!7$M keys per node). 
The values are 2K big, yielding roughly 46GB of actual data, replicated three-way in HDFS. The keys are hash-partitioned
across the servers. The data accesses are 50\% reads and 
50\% writes. The key access frequencies follow a Zipf distribution, generated following 
the description in~\cite{Gray:1994:QGB:191839.191886}, with $\theta=0.8$, which we derive from production 
workloads. \inred{In this distribution, the first key is accessed XXX\% of the  time.}
%in Yahoo's deployment of Omid~\cite{Omid2017}. 
%Note that under this access distribution, data requests are load-balanced across nodes.

\noindent
We test the system with two transaction mixes:
\begin{description}
\item[Random mix] -- 
transaction sizes (number of reads and writes) follow a Zipf distribution with $\theta=0.99$, with a cutoff at $10$. 
With these parameters, $63\%$ of the transactions access three keys or less, and only $3\%$ access $10$ keys. 
We vary the system load from 30K to 500K transactions per second (tps). 
\item[BRWC] -- $80\%$ of the transactions are drawn from the random mix distribution, and $20\%$ perform a read
and then a write to the same key. 
\end{description}

We add the BRWC workload since single-key read+write transactions are common in production, but are highly unlikely to 
occur in our random mix, which uses random key selection with billions of keys.

\begin{figure}[htb]
	\centering
      	\includegraphics[width=0.48\textwidth]{figs/throughputlatency1.pdf}
	    \caption{Throughput vs.\ latency, transaction size = 1.}
        \label{fig:tl-1}      
\end{figure}


\begin{figure*}[hbt]
\centering
\begin{tabular}{ccc}
      \begin{subfigure}[t]{0.48\textwidth}
         \includegraphics[width=\textwidth]{figs/latency_allPUTGET.pdf}
        \caption[]{Low load (100K tps).}
        \label{fig:stack-brc}

      \end{subfigure} 
    
& 
      \begin{subfigure}[t]{0.48\textwidth}
      	\includegraphics[width=\textwidth]{figs/latencyHighThrough_PUTGETRMW.pdf}
	\caption{High load (500K tps).}
	\label{fig:hightx}
      \end{subfigure}  & 

\end{tabular}
       \caption{Latency breakdown  for single-key transactions under  random mix workload. }
\end{figure*}


\subsection{Results}
\label{ssec:results}

\paragraph{Throughput and latency of short transactions.} 

Recall that \sys\ is motivated by the prevalence of short transactions in production, and is designed with
the goal of speeding such transactions up.
Its advantage is less pronounced for long transactions, where the cost of begin and commit is amortized
across many reads and writes.
To make the comparison meaningful, we classify transactions by their lengths and whether they
access a single key, and study each transaction class separately. 

We begin with short transactions. 
Figure~\ref{fig:tl-1} presents the average latency of single-key transactions run as part of the random mix,
as a function of {system} throughput.
Figure~\ref{fig:stack-brc}  then zooms in on the latency of such transactions under 
a throughput of 100K tps, and breaks up the different factors contributing to it. 
Figure~\ref{fig:hightx}  presents a similar breakdown under a high load of 500K tps; Omid 
is not included in the latter since it does not sustain such high throughput.

As we can see, under light load, \sys\ improves the latency of Omid by 4x to 5x, even without the fast path.
This is because in Omid, both begin and commit wait for preceding transactions to complete the writes of 
their commit entries; this stems from Omid's design choice to avoid the need for resolving pending write intents
by aborting transactions; see last (rightmost) column in Table~\ref{table:design-space}. 
Single-key writes suffer from both the begin and commit latencies, whereas single-key reads  
suffer only from begins (Figure~\ref{fig:stack-brc}). 

As load increases, Omid suffers from a well-pronounced bottleneck, and its latency at 250K tps is doubled, where \sys's 
latency is unaffected. The extra delay in Omid is due to batching of commit record updates, 
which its TM applies to handle congestion~\cite{Omid2017}. 
\sys, in turn, begins to experience a bottleneck (due to a heavy request load on the TM)  at 400K tps.
Such congestion arises due to Omid's centralized 
commit entry management (penultimate column in Table~\ref{table:design-space}).

The FP API delivers better performance for this traffic. For instance, under low load (Figure~\ref{fig:stack-brc}),
single writes take an average of 2.4ms using 
the {\code bwc} API versus 5.65ms using the regular transaction API (consisting of begin, write, and commit). 
For comparison, a native HBase write takes roughly 2ms under this load.
A single read executed using {\code brc} takes 1.5ms, which is the average latency of a native HBase read,
versus 2.5ms as a regular transaction. 
For transactions that read and write a single key as part of the BRWC workload, 
%Their latency breakdown under a 100K tps throughput is shown in Figure~\ref{fig:rmw}.
the fast path implementation (consisting of \code{br} and \code{wc} calls) completes within 3.9ms,
versus 6.5ms for Vanilla \sys\ and 37.9ms for Omid. 
%
Under high load (Figure~\ref{fig:hightx}), the fast path is more beneficial: it reduces the latency of write
by a factor of 2.7, from 6.9ms  to 2.5ms, and the read latency by a factor of 2.2, from  3.65ms to 1.6ms.
%, read, and brwc from 11.24ms, 8.24ms, and 11.85ms to 2.4ms, 1.29ms, and 4.19ms respectively. 

As for 2PC, the performance is same as \sys\ for throughput up to $500K$ tps. Above $500K$ tps the centrlized conflict analysis becomes a bottlneck and 2PC has a minor speedup of 1.1x over \sys.

\paragraph{Long transactions.} 
We now examine longer transactions run as part of the random mix workload.
Figure~\ref{fig:throughput-latency} shows the results for transactions of lengths $5$ and $10$.
We see that the latency gap of \sys\ over Omid remains similar, but is amortized 
by other operations. Omid's control requests (begin and commit) continue to 
dominate the delay, and comprise $61\%$ of the latency of 10-access transactions.
In contrast, \sys's transaction latency is dominated by data access. For example, in 10-operation transactions only 
$17\%$ of the time is spent on the control path, which leads to much faster completion. 



\begin{figure*}[t]
\centering{
\begin{tabular}{cc}

    \begin{subfigure}[t]{0.48\textwidth}
	\includegraphics[width=\textwidth]{figs/throughputlatency10.pdf}
	\caption[]{Throughput versus latency, transaction size = 10}
    \label{fig:tl-10}
  \end{subfigure} & 


  \begin{subfigure}[t]{0.48\textwidth}
	\includegraphics[width=\textwidth]{figs/latency_5_10.pdf}
	\caption[]{Latency breakdown, transaction size = 5, 10}
    \label{fig:stack-tx10}
  \end{subfigure} 
\end{tabular}  	
}		
  \caption{Latency vs.\ throughput  and latency breakdown  for long transactions in random mix workload. }
  \label{fig:throughput-latency}
\end{figure*}

Nevertheless,
the FP mechanism takes its toll on the data path, which resorts to atomic check\&mutate operations 
instead of simple writes. This is exacerbated for long transactions. 
For example, a 10-access transaction takes 24.7ms with FP \sys, 
versus 21.6ms with Vanilla \sys. The performace of 2PC with long transactions is the same as FP \sys\ because it also uses check\&mutate operations during the voting phase of a commit.



\begin{figure}[h!]
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centerline{
\includegraphics[width=.9\textwidth]{figs/low_speedup.pdf}
}
\caption{Low load (100 tps)} 
\label{fig:slowdown-low}
\end{subfigure} 
\begin{subfigure}[t]{0.48\textwidth}
\centerline{
\includegraphics[width=.9\textwidth]{figs/high_speedup.pdf}
}
\caption{High load (500 tps)} 
\label{fig:slowdown-high}
\end{subfigure} 
\caption{Latency speedup with  fast path API in {\sys}.}
\label{fig:fp-tradeoff}
\end{figure}



Figure~\ref{fig:fp-tradeoff} summarizes the tradeoffs entailed by the fast path API
(relative to Vanilla \sys) for the different transaction classes. 
We see that under low load (Figure~\ref{fig:slowdown-low}),
the speedup for single-write transactions is 2.3x, whereas the worst slowdown is $13\%$. 
In systems geared towards real-time processing, this is a reasonable tradeoff, since long transactions 
are infrequent and less sensitive to extra delay.
Under high load (Figure~\ref{fig:slowdown-high}), the fast path is clearly advantageous, most notably, the speedup for writes exceeds $170\%$.
%In systems geared towards real-time processing, this is a reasonable tradeoff, since long transactions 
%are infrequent and less sensitive to extra delay. Under the studied workload, e.g.,   FP \sys\/ is more desirable. 



\paragraph{Abort rates.}

We note that \sys\/ yields slightly higher rates of transaction aborts compared to Omid 
(recall that Vanilla \sys\/ aborts tentative writes in favor of concurrent reads, whereas FP \sys\/ also aborts
singleton writes in presence of concurrent tentative writes). However, the abort rates exhibited by all  
the systems are minor. Here, under the highest contention, FP \sys\/ aborts approximately $0.1\%$ 
of the transactions vs Vanilla \sys's $0.08\%$ and Omid's $0.07\%$ 
(the latter  is in line with~\cite{Omid2017}).  
\remove{
}
