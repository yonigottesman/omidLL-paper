% Transactions in big-data platforms
In recent years, transaction processing~\cite{Gray:1992:TPC:573304} technologies have paved their way into many-petabyte big data 
platforms~\cite{Spanner2012,Percolator2010,Omid2017}. 
%In some cases, they are built into the storage system itself~\cite{Spanner2012} whereas in the others they are standalone services~\cite{Omid2017}. 
In a number of industrial systems~\cite{Percolator2010,Omid2017,tephra,cockroach}, transaction management complements 
existing underlying key-value storage with {\em atomicity}, {\em consistency}, {\em isolation\/} and 
{\em durability} (ACID) semantics that enable programmers perform 
complex data manipulation without over-complicating their applications. Transaction awareness 
in web-scale applications started from specific use cases like real-time content indexing~\cite{Percolator2010,
Omid2017} but quickly expanded to 
%generic high-level abstractions, e.g., 
full-scale SQL OLTP and online analytics~\cite{Phoenix, F1-2013}.

Similarly to many technologies, the adoption of transactions took a  ``functionality-first" trajectory. 
For example, the developers of Google Spanner~\cite{Spanner2012} wrote: ``We believe it
is better to have application programmers deal with performance problems due to overuse 
of transactions as bottlenecks arise, rather than always coding around the lack of transactions''. 
Yet the expectation for high performance is rapidly picking up. %For instance, 
Whereas early 
%In the early days,  
transaction systems were throughput-oriented~\cite{Percolator2010, Omid2017}, 
with the thrust into new interactive domains like messaging~\cite{Borthakur:2011} and algorithmic 
trading~\cite{opentsdb}, latency becomes essential. This paper is motivated by such  applications.

Consider, for example, the platform powering Yahoo! Mail, which serves hundreds of millions of users. 
The Mail backend ingests billions of messages daily; messages undergo both machine 
classification, (e.g., for spam filtering, thread detection, and ``smart view'' tagging\footnote{\footnotesize{\url{
https://yahoohelpcommunity.tumblr.com/post/118485031125/getting-to-know-smart-views-in-yahoo-mail}}}),
and user manipulation (e.g., starring, tagging, and moving between folders).    
Mail users browse their mail and search for content, issuing many billions of requests a day; they
expect a consistent experience, where messages never 
disappear, starred content gets prioritized in search, folder counters are reliable, etc.

The Yahoo! Mail metadata and search platform is built on top of Apache HBase~\cite{hbase}, 
which provides reliable and scalable key-value storage. The system's first generation
was built without transaction support. This exposed  developers to very complex programming scenarios 
to achieve ACID behavior, (e.g., consistent folder listings, atomic updates of multiple counters). 
%We would like to build the next generation of the system using a transaction API on top of HBase.
We have looked into building the next generation using 
 Apache Incubator Omid~\cite{omid}, which provides a transactional API on top of HBase, and 
 is already in-use in other Yahoo systems~\cite{omid-blog}.
Unfortunately, while this makes programming much easier, it also jeopardizes 
the real-time latency SLA for interactive user experience. For example,  
simple updates and point queries must complete within single-digit milliseconds, 
whereas Omid, which is designed for high-throughput data pipelines~\cite{Omid2017}, 
induces a latency of tens, or even hundreds of milliseconds under high loads. 

Motivated by this example, we have developed {\sys\/} -- a low-latency {\em and\/} high-throughput 
transaction processing system for Apache HBase. Similarly to other modern transaction managers~\cite{cockroach,Spanner2012,Percolator2010,Omid2017},
{\sys\/} implements the {\em snapshot isolation} (SI) consistency model~\cite{DBLP:conf/sigmod/BerensonBGMOO95},
which scales better than traditional serializability implementations. {\sys\/} is based on Omid~\cite{omid}, 
and dissipates the principal bottleneck present therein.
%in the prior implementations the overhead of begin and commit operations. 
Its advantage is maximized for short  transactions, which are prevalent in latency-sensitive applications,
and \sys\ can process \inred{in a handful of milliseconds}. The new protocol is also favorable for system throughput. 
%, which is \inred{an order of magnitude higher than} previously achievable limits. 
%Different aspects of our protocol are inspired by other systems, while their combination is novel.
%, to the best of our knowledge. 

{\sys\/} further introduces a novel {\em fast path\/} algorithm for short (single-key) transactions 
that eliminates the begin/commit overhead entirely. The fast path requires minor extensions to the underlying 
data store, to enable local commits directly within the storage layer and verify the correctness of general 
transactions in presence of such commits. This improves the latency of short transactions 
\inred{by roughly 70\%} at the cost of minor (\inred{10-20\%}) negative impact on longer transactions. 
The fast path is orthogonal to other protocol aspects, and can be  supported in other 
transaction processing services.

We implemented {\sys\/} based on the open source Omid code, and also extended the HBase code to 
enable fast path transactions. Our experiments show substantial reduction in latency for all transactions, 
as well as near-infinite system scalability. Fast path transactions perform \inred{almost as fast as  
native atomic operations}, and their impact on longer transactions is insignificant. 

% \Idit{The roadmap below is not essential; maybe replace with summary of contributions.}
The remainder of this paper is organized as follows:
In Section~\ref{sec:api} we define the  API and semantics of a transaction processing service. 
Section~\ref{sec:ll} presents \sys, without the fast path. 
Section~\ref{sec:alg} then describes our support for fast path  transactions in \sys.  
Section~\ref{sec:eval} presents an empirical evaluation.
In Section~\ref{sec:context} we generalize the fast path algorithm, and explain how it could be implemented in 
other systems. We review related work in Section ~\ref{sec:related} and conclude with Section~\ref{sec:conclusions}.
