

Comprehensive SQL support in Phoenix involved functionality extensions as well 
as performance optimizations. 

Performance-wise, 
Phoenix extensively employs stored procedures implemented as HBase coprocessors
in order to eliminate the overhead of multiple round-trips to the data store. 
We integrated Omid's code within such HBase-resident procedures.
For example, Phoenix coprocessors invoke transactional reads, and so we implemented Omid's 
transactional read -- the loop implementing read in Algorithm~\ref{fig:get-pseudocode} -- as a coprocessor as well. 
This allowed for a smooth integration  with Phoenix and also reduced the overhead of transactional
reads when multiple versions are present.

Functionality-wise, we added support for on-the-fly construction of secondary indexes
(see Section~\ref{ssec:indexes}) and extended 
Omid's SI model to allow for multiple read- and write-points, as required 
in some Phoenix applications (see Section~\ref{ssec:snapshot})\footnote{\small{\url{https://issues.apache.org/jira/browse/OMID-82} summarizes the new features in Omid.}}. 

\subsection{Index construction}
\label{ssec:indexes}

A secondary index in SQL is an auxiliary table that provides fast access to data in a table 
by a key that is different from the table's primary key. This need often emerges in analytics scenarios, in
which data is accessed by multiple dimensions. Typically, the secondary key serves as the 
primary key of the secondary index, and is associated with a unique reference into the base table 
(e.g., primary key + timestamp). SQL query 
optimizers exploit secondary indexes in order to produce efficient query execution plans. Query speed 
is therefore at odds with update speed since every write to a table triggers writes to all its indexes. 

The SQL standard allows creating indexes on demand. When a user issues the {\sc {Create Index}} 
command, the database (1) populates the new index table with historic data from the base table, and
(2) installs a trigger to augment every new write to the base table with a write to the index table. 
It is desirable to allow temporal overlap between the two, in order to avoid stalling  writes while 
the index is being populated. 

We exploit Omid's SI semantics in order to create such indexes. To achieve (1), 
Phoenix invokes a transaction that scans a snapshot of the base table and 
streams the data into the  index table. This way, historic data is captured without 
blocking concurrent puts. {\inred{Phoenix has two types of indexes: global (sharded 
independently from the base table) and local (co-sharded with the base table). 
In the latter case, the whole transaction is pushed into a coprocessor; this 
local distributed execution speeds up the process significantly.}}

Once the bulk population completes, the index can become available to queries. 
To achieve (2), the database creates a trigger that augments all transactions 
that update the base table with an additional update of the secondary index. 
 
In order to guarantee the new index's consistency with respect to the base table, the snapshot creation 
and the trigger setup must be atomic. In other words, all writes beyond the snapshot   
timestamp must be handled by the trigger. Omid achieves this through a new {\em fence\/} API
implemented by the TM, which is  invoked when the trigger is installed. 
A fence call produces a new \emph{fence timestamp} by fetching-and-incrementing the TM's clock, and 
records the fence timestamp in the TM's local state.  
Subsequently, the TM aborts every transaction whose read timestamp precedes the fence's 
timestamp and attempts to commit after it. 
Note that there is at most one fence timestamp per index at a given time.


Neither the bulk index population nor its incremental update require write conflict detection 
among the index keys, for different reasons. The former does not contend with any other 
transaction, and hence is committed automatically -- the commit cells are created simultaneously
with the rest of the index data. The latter is voided by the TM detecting conflicts at the base 
table upon commit. Hence, in transactions augmented with secondary index updates,  
there is no need to embed the added index keys in the commit 
request, and so the load on the TM does not increase. 
Omid provides extensions for its put API for this scenario. 

Note that the above mechanism is not unique for secondary indexes. 
It can be applied to additional types of derived data, e.g., materialized views. 

\subsection{Extended snapshot semantics}
\label{ssec:snapshot}

Some Phoenix applications require semantics that  
% prefer to avoid observing  writes made by the current transaction -- a deviation 
deviate from the standard SI model in that a transaction does not see (all of) its own writes.
We give two examples. 
%, which provides the ``read-your-own-writes'' semantics. 

\mypara{Checkpoints and snapshots.} 
Consider a social networking application that stores its adjacency graph 
as a table of neighbor pairs. The transitive closure of this graph is computed in
multiple iterations. Each iteration scans the table, computes new edges to add to the 
graph, and inserts them back into the table. Other functions like PageRank are implemented
similarly. Such programs give rise to the pattern given in Figure~\ref{alg:six}. 

Note that the SQL statement loops over entries of $T$ (in other words, the program is a nested loop). 
{\inred{Normally, the SI model enforces the read-your-own-writes semantics. However, in this case}}
the desirable semantics are that the reads only see  data that existed prior to the 
current statement's execution. This isolation level is called 
\emph{snapshot isolation exclude-current (SIX)}.

To support this behavior, we implement in Omid two new methods, {\em snapshot\/} and {\em checkpoint}.
Following a checkpoint, we move to the SIX level, where reads see the old snapshot (i.e., updates that
precede the checkpoint call) and writes occur in the next snapshot, which is not yet visible.
The snapshot call essentially resets the semantics back to SI, making all previous writes visible.
Given these two calls, Phoenix translates SQL statements as above to the loop in Figure~\ref{alg:six}-- 
it precedes the evaluation of the nested SQL statement with a checkpoint, and follows it with a snapshot.
Thus, each SQL statement sees the updates of the preceding iteration but not of the current one. 

\begin{figure}
\begin{subfigure}[tb]{\columnwidth}
\underline{\hspace{\columnwidth}}
\underline{\small SQL program that requires SIX isolation:}
\begin{algorithmic}
\For{iterations $i=1, \dots$}
\State insert into T as select func(T.rec) from T  where ... 
\EndFor
\end{algorithmic}
\end{subfigure}
\begin{subfigure}[tb]{\columnwidth}
\ \\ \\
\underline{\small Calls generated by Phoenix for the SQL program:}
\begin{algorithmic}
\For{iterations $i=1, \dots$}
\State checkpoint \Comment move to SIX isolation
\State iterator $\leftarrow$ T.scan(\dots) \Comment scan in current snapshot
\ForAll{rec $\in$  iterator.getNext()} \Comment parallel loop
    \State T.put(func(rec)) \Comment write to next snapshot              
\EndFor
\State snapshot  \Comment make all previous writes visible
\EndFor
\end{algorithmic}
\underline{\hspace{\columnwidth}}
\end{subfigure}
\caption{  SQL program requiring SIX isolation and corresponding Phoenix execution plan.}
\label{alg:six}
\end{figure}

To support the SIX isolation level in Omid, the TM promotes its transaction timestamp counter $ts_r$ 
in leaps of some $\Delta > 1$ (rather than $1$ as in the legacy system).  The client manages 
two distinct local timestamps, $\tau_r$ for reads and $\tau_w$ for writes. By default, 
it uses  $\tau_r = \tau_w = ts_r$, which achieves the traditional SI behavior. Omid 
then has the new methods, snapshot and checkpoint, increment 
$\tau_r$ and $\tau_w$, respectively. If the consistency level is set to SIX, 
it maintains 
\[
\tau_w = \tau_r+1 < ts_r+\Delta,
\]
thereby separating the reads from the writes. 
A transaction can generate $\Delta-1$ snapshots without compromising
 correctness. By default, Omid uses $\Delta=50$.  

\mypara{Snapshot-all.} 
When a transactions involves multiple snapshots that update the same key, 
it is sometimes required to read \emph{all} versions of key that existed 
during the transaction. This is called the \emph{snapshot-all} isolation level.

We use this isolation level, for example, when aborting a transaction that 
involves multiple updates to a key indexed by a secondary index. 
Consider a key $k$ that is initially associated with a value $v_0$, 
and a secondary index maps $v_0$ to $k$. 
When a transaction updates $k$ to take value $v_1$, a tentative deletion marker
is added to $v_0$ in the secondary index while $v_1$ is added. 
If the transaction then updates 
the same key (in an ensuing snapshot) to take the value $v_2$, 
then $v_1$ is marked as deleted and $v_2$ is added, and so on. 
In case the transaction aborts, we need to roll-back all deletions. 
But recall that index updates are not tracked in the write-set,
and so we need to query the data store in order to discover the set
of updates to roll back. We do this by querying $k$ in the primary table
with the snapshot-all isolation level, to obtain all values 
that were associated with $k$ throughout the transaction, beginning with $v_0$.
We can then clean up the redundant entries in the secondary index, and 
remove the deletion marker from $v_0$.
 
