%We present \sysll, a scalable low-latency TPS algorithm 
% on top of HBase. 
%\sys\ is an evolution of the open-source Omid TPS, redesigned to reduce latency.
%For clarity of the presentation, in this section we describe \sys\ without the fast-path; 
%This protocol 
%that satisfies standard SI semantics and is amenable to multi-tenancy.
%We saw above that, while many TPSs follow a similar schema,  they make different design choices when implementing this schema. 
We overview \sysll's design choices in Section~\ref{ssec:ll-txns} 
and give  the protocol
in Section~\ref{ssec:ll}.
Section~\ref{ssec:ll-graphs} presents exemplary performance results.  
%showing the advantage of \sysll\ over Omid. 

\subsection{\sysll\ design choices}
\label{ssec:ll-txns}

Our design choices  are geared towards high performance without sacrificing cloud deployability.
Table~\ref{table:design-space} compares our  choices with ones made in other TPSs. 
%We are not familiar with another TPS that makes the same design choices as \sysll. 


\begin{table}[htb]
\small
\centerline{
\begin{tabular}{|l|ccc|}
% data store modification & 
\hline
TPS & validation & commit entry &  multi \\
  &  &  updates  &  tenancy   \\
\hline
Percolator & {\bf D} & {\bf D} & no  \\
CockroachDB	& {\bf D}      & {\bf D} & yes  \\
Omid1, Tephra 	& {\bf C}      & {\bf R} & yes  \\
Omid  		& {\bf C}      & {\bf C} & yes \\
{\bf \sysll }		& {\bf C}      & {\bf D} & {\bf yes}  \\
% \sys			& {\bf C} & {\bf D} & {\bf D} & yes& yes\\
\hline
\end{tabular}
}
\caption{Design choices  in TPSs. {\bf C} -- centralized,  {\bf D} -- distributed, {\bf R} -- replicated.}
\label{table:design-space}
\vspace{-0.5cm}
\end{table}



\mypara{Centralized validation.}
Snapshot isolation requires validating write-sets -- i.e., checking for write-write conflicts -- upon commit. 
\sysll\ adopts Omid's centralized conflict detection mechanism, which eliminates the need for locking objects
in the data store, and is extremely scalable~\cite{Omid2017}. 

Other TPSs (like Percolator and  CockroachDB~\cite{cockroach}) instead use a distributed 2PC-like protocol that locks all written objects during validation (either at commit time or during the write). To this end, they use atomic check\&mutate operations on the underlying data store. This slows down either commits (in case of commit-time validation) or transactional writes (in case of write-time validation), which takes a toll on long transactions, where validation time is substantial. 

\mypara{Distributed commit entry.}
%
Because a transaction may write many keys and must commit them all in one atomic data store operation,  
TPSs keep per-transaction \emph{commit entries}, which are the source of truth regarding the transaction status 
(pending, committed, or aborted). 
The early generation of Omid~\cite{OmidICDE2014} (referred to as Omid1) and Tephra replicate commit entries 
of pending transactions among all active clients, which consumes high bandwidth and does not scale. Omid 
instead uses a dedicated \emph{commit table (CT)}, and 
has the centralized TM persist all commits to this table. 

Our experiments show that the centralized access to commit entries is Omid's main scalability bottleneck, 
and while this bottleneck is mitigated via batching, this also increases latency.
Omid chose this  option as it was designed for high throughput. 
%
Here, on the other hand, we target  low latency. 
We therefore distribute the commit entry updates, and allow commit entries of different transactions to be 
updated in parallel by independent clients. 
We further shard this table evenly across many nodes in order to spread the load.
The distribution of commit entry updates reduces latency
by up to an order of magnitude for small transactions.



\mypara{Multi-tenancy.}
We note that commit table updates are distributed also in CockroachDB and Percolator. 
The latter takes this approach one step further, and distributes not only the commit table updates 
but also the actual commit entries. There, commit entries reside in user data tables.
%where the first row written in a given transaction holds the commit entry for that transaction.
The problem with this approach is that it assumes that all clients have permissions to access all 
tables. For example, a transaction attempting to read from  table $A$ may encounter a tentative (non-committed) write  
produced by a transaction that accessed table $B$ before table $A$, and will need to refer to 
that transaction's commit entry in table $B$ in order to determine its status. This approach 
did not pose a problem in Percolator, which was designed for use in a single application (Google Search), 
but is unacceptable in multi-tenant settings.   

Unlike data tables, which are owned by individual applications that manage their permissions,  
the dedicated commit table is owned by the TPS; it is accessed exclusively by the 
TPS client library, which in turn is only invoked   internally by the database engine, and not by application code. 




