%% \begin{figure*}[!h]
%%   \centering
%%   \begin{subfigure}[t]{0.3\textwidth}
%% 	\includegraphics[width=\textwidth]{figs/read_latency.pdf}
%% 	\caption[]{Read}
%%     \label{fig:latency:read}
%%   \end{subfigure}
%%   \begin{subfigure}[t]{0.3\textwidth}
%% 	\includegraphics[width=\textwidth]{figs/write_latency.pdf}
%% 	\caption[]{Write}
%%         \label{fig:latency:write}
%%   \end{subfigure}	
%%   \begin{subfigure}[t]{0.3\textwidth}
%% 	\includegraphics[width=\textwidth]{figs/rmw_latency.pdf}
%% 	\caption[]{Read Modify Write}
%%     \label{fig:latency:rmw}
%%   \end{subfigure}			
%%   \caption{Breakdown of transaction latency}
%%   \label{fig:latency}
%% \end{figure*}


\begin{figure}
\caption{\bf{Experiment architecture.}}
\label{fig:experiment}
\end{figure}

\begin{figure*}[]
  \centering
  \begin{tabular}{cccc}
    
  \begin{subfigure}[t]{0.4\textwidth}
	\includegraphics[width=\textwidth]{figs/singlewrite.pdf}
	\caption[]{Single write transactions}
    \label{fig:latency:lorra1write}
  \end{subfigure} &

  \begin{subfigure}[t]{0.4\textwidth}
	\includegraphics[width=\textwidth]{figs/singleread.pdf}
	\caption[]{Single read transactions}
    \label{fig:latency:lorra1read}
  \end{subfigure} \\

    \begin{subfigure}[t]{0.4\textwidth}
	\includegraphics[width=\textwidth]{figs/lorravslltx5.pdf}
    \caption[]{Transaction size 5}
    \label{fig:latency:read}
    \label{fig:latency:lorra5}
  \end{subfigure} &

  \begin{subfigure}[t]{0.4\textwidth}
	\includegraphics[width=\textwidth]{figs/lorravslltx10.pdf}
    \caption[]{Transaction size 10}
    \label{fig:latency:lorra10}
  \end{subfigure} \\
  
    
  \end{tabular}
  \caption{Latency of \sys\ and \sysll\ }
\end{figure*}


\begin{figure}[!h]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
	\includegraphics[width=\textwidth]{figs/omdLL1.pdf}
	\caption[]{Transaction of size 1 latency}
    \label{fig:ll:tx1}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\textwidth}
	\includegraphics[width=\textwidth]{figs/omdLL5.pdf}
	\caption[]{Transaction of size 5 latency}
    \label{fig:l:tx5}
  \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
	\includegraphics[width=\textwidth]{figs/omdLL10.pdf}
	\caption[]{Transaction of size 10 latency}
    \label{fig:l:tx10}
  \end{subfigure}			
  \caption{A Comparison between Omid's centralized transaction entry and \sys's distributed transaction entry.}
  \label{fig:ll}
\end{figure}

We now evaluate {\sys}'s performance for both the generic and the fast-path transactions. 
We compare {\sys\/} to the mainstream Omid implementation. Both systems use HBase 
as key-value storage backend. 

We emulate a very large system with a small cluster, as follows. We assume that the keys 
are hashed i.i.d. to the region servers, i.e., the server load is roughly balanced. The region
servers run embarrassingly parallel, whereas the TM  is the only centralized service in the 
system. Therefore, an experiment that generates a fraction of load for a small number of 
region servers and a full load of begin/commit requests on the TM truthfully captures 
the average end-to-end latency. 

We use a 3-node HBase cluster to serve end-to-end transactions. In parallel, we generate a background 
begin/commit load on the TM that is worth of a 1000-node cluster. Our experiment employs a popular
YCSB workload driver~\cite{Cooper:2010:BCS:1807128.1807152} that exercises the {\sys} (respectively, 
Omid) API's to generate the conventional transaction traffic; we measure their latency. 
A custom tool generates low-level background begin and commit requests to the TM~\cite{Omid2017}.
Figure~\ref{fig:experiment} depicts our evaluation platform. 

The core dataset size is 50GB with 3-way replication. 

\remove{
\paragraph{Methodology}

To evaluate \sys, we set up a 3 node Hbase cluster loaded with 50GB of data, and a machine hosting the TM.
Another machine created load on the system by running threads that generated random transactions towards Hbase and the TM. 

Transaction size had a Zipf distribution from 1 to 10.
To measure the latency of transactions we modified YCSB~\cite{Cooper:2010:BCS:1807128.1807152} to perform a begin and commit towards the TM at the beginning and end of each transaction, and used YCSB framework to measure the latency of each stage.
}

\paragraph{Distributed transaction entry}
We first compare \sys's distributed transaction entry approach to Omid's centralized approach. Figure~\ref{fig:ll} shows the latency of a transaction while applying increasing load on the system. The load is measured in kilo transactions per second (KTPS), and the transaction latency is measured in milliseconds. Figure~\ref{fig:ll:tx1} shows the latency of a transaction with a single read or write. The figure shows that latency obtained by \sys\ is \speedup{4} faster than Omid 2. \sys\ performs better because Omid 2 is throughput oriented and batches the writes to transaction entries, so on every begin stage the client has to wait for all commits in the batch to get persistent.
Figure~\ref{fig:ll:tx1} shows the latency of transactions with 5 read or write operations. This time the latency obtained by \sys\ is \speedup{1.8} faster than Omid 2. For transactions with 5 operations to an Hbase table, the begin and commit time become negligible.

\paragraph{FP API latency}
Figure~\ref{fig:latency} compares the latency observed by YCSB when running regular and FP transactions.
We focus on 3 types of transactions: (1) Read, (2) Write and (3) Read modify write.

Figures~\ref{fig:latency:read} and \ref{fig:latency:write} show the latency of a single read and write transaction. Regular transactions (b-r-c, b-w-c) query the TSO at the begin and commit stage which add 0.3ms for read transactions and 3ms for write transactions.
Figure~\ref{fig:latency:rmw} compares the latency of a read modify write transaction, once using the FP API (br-bwc) and once using the regular transaction API (b-r-w-c).The begin and commit stage of the regular transaction add 3ms to each transaction.







\Yoni{
  --background noise
  --switch order of charts
  --explain that the LL is better for all TPS so it doesn't matter the point we choose to check}
